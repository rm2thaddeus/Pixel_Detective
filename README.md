<!-- Sprint-11 requirements summary:
Functional: interactive latent-space explorer with clustering (DBSCAN, K-Means, Hierarchical), lasso selection â†’ collection workflow, thumbnail previews, and cluster quality metrics.
Non-functional: <3 s load for 1 k points on GPU, accessibility â‰¥90 %, responsive mobile layout, memory <100 MB.
-->
# Pixel Detective - AI-Powered Media Search Engine

A sophisticated, locally-hosted media search platform that leverages cutting-edge AI models to provide intelligent search capabilities across personal media libraries. Built with a modern microservices architecture featuring FastAPI backends and a Next.js frontend.

## ğŸ¯ Project Overview
Pixel Detective is a vibe coding manifesto: every aspect of this project was created through a process of idea generation, prompt engineering, and AI-driven software synthesisâ€”no hand-written code, just pure concept-to-execution via prompts.

### Key Achievements
- **ğŸš€ Complete Architecture Refactor** - Migrated from monolithic to microservices architecture
- **âš¡ High-Performance Backend** - GPU-optimized ML inference with dynamic batching
- **ğŸ¨ Modern Frontend** - React/Next.js application with Chakra UI
- **ğŸ“Š Vector Database Integration** - Qdrant for persistent, scalable vector storage
- **ğŸ”§ DevOps Pipeline** - Docker containerization and MCP server integration
- **ğŸ—ºï¸ Interactive Latent Space Explorer** - Real-time UMAP scatter plot with clustering, lasso selection & thumbnail previews
- **âš¡ GPU-Accelerated UMAP & Clustering** - Dedicated RAPIDS cuML micro-service delivering 10-300Ã— speed-ups
- **ğŸ¬ WebGL Timeline Visualization** - CUDA-accelerated code evolution timeline with real-time commit progression

## ğŸ—ï¸ Architecture

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   Next.js       â”‚
                    â”‚   Frontend      â”‚
                    â”‚                 â”‚ 
                    â”‚ â€¢ React/TS      â”‚
                    â”‚ â€¢ Chakra UI     â”‚
                    â”‚ â€¢ State Mgmt    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚                  â”‚                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚   Ingestion     â”‚  â”‚   GPU-UMAP      â”‚  â”‚   Qdrant    â”‚
â”‚  Orchestration  â”‚  â”‚   Service       â”‚  â”‚  Vector DB  â”‚
â”‚   (Port 8002)   â”‚  â”‚  (Port 8003)    â”‚  â”‚ (Port 6333) â”‚
â”‚                 â”‚  â”‚                 â”‚  â”‚             â”‚
â”‚ â€¢ Coordination  â”‚  â”‚ â€¢ RAPIDS cuML   â”‚  â”‚ â€¢ Storage   â”‚
â”‚ â€¢ Collections   â”‚  â”‚ â€¢ CUDA Accel    â”‚  â”‚ â€¢ Search    â”‚
â”‚ â€¢ Metadata      â”‚  â”‚ â€¢ Clustering    â”‚  â”‚ â€¢ Metadata  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  ML Inference   â”‚
â”‚  Service        â”‚
â”‚  (Port 8001)    â”‚
â”‚                 â”‚
â”‚ â€¢ CLIP Models   â”‚
â”‚ â€¢ BLIP Caption  â”‚
â”‚ â€¢ Embeddings    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸŠ Current Implementation Status

**âœ… PRODUCTION READY** - Sprint 11 Complete  
**ğŸš€ Interactive Latent Space Explorer** - Real-time UMAP visualization with clustering  
**âš¡ GPU Acceleration** - RAPIDS cuML for 10-300Ã— performance boost  
**ğŸ¯ Smart Architecture** - Frontend auto-detects GPU vs CPU clustering services  

### **Core Services Running:**
- **Frontend** (3000) - Next.js with advanced DeckGL visualization
- **Ingestion API** (8002) - Collection management and orchestration  
- **GPU-UMAP Service** (8003) - RAPIDS cuML clustering (Docker)
- **ML Inference Service** (8001) - CLIP/BLIP models (Host Python)
- **Qdrant Database** (6333) - Vector storage and search

## âœ¨ Core Features

### AI-Powered Search
- **Natural Language Queries** - Search using descriptive text
- **Visual Similarity** - Find images using reference images  
- **Automatic Captioning** - BLIP model generates descriptions
- **Semantic Embeddings** - CLIP model for deep understanding

### High-Performance Processing
- **GPU Acceleration** - CUDA-optimized inference pipeline
- **Batch Processing** - Efficient handling of large image sets
- **Async Operations** - Non-blocking UI with background processing
- **Memory Management** - Smart model loading/unloading

### Modern Web Interface
- **Responsive Design** - Mobile-first, accessible UI
- **Real-time Updates** - Live progress tracking and notifications
- **Collection Management** - Organize and manage image collections
- **Curation Actions Menu** - Launch near-duplicate scans and view ingestion logs
- **Advanced Filtering** - Metadata-based search refinement

### Code Evolution Visualization
- **WebGL Timeline** - CUDA-accelerated commit progression visualization
- **Dual Rendering Engines** - SVG (detailed) and WebGL (performance) modes
- **Interactive Timeline** - Play/pause with real-time commit highlighting
- **Adaptive Performance** - Automatic node budget adjustment based on device capabilities
- **Sprint Analytics** - Connect commits to development sprints and track progress

## ğŸ› ï¸ Technology Stack

### Backend
- **FastAPI** - High-performance async web framework
- **PyTorch** - Deep learning model inference
- **Qdrant** - Vector similarity search database
- **Docker** - Containerized deployment

### Frontend  
- **Next.js 14** - React framework with App Router
- **TypeScript** - Type-safe development
- **Chakra UI** - Modern component library
- **Zustand** - Lightweight state management
- **WebGL2** - GPU-accelerated timeline visualization

### AI/ML Models
- **CLIP** - Vision-language understanding
- **BLIP** - Image captioning
- **Custom Pipelines** - Optimized inference workflows

## ğŸš€ Getting Started

### Prerequisites
- Node.js 18+ and npm
- Python 3.9+
- Docker & Docker Compose
- NVIDIA GPU with CUDA (recommended)

### Quick Start

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/vibe-coding.git
cd vibe-coding
```

2. Follow one of the workflows below:

Choose one of the following workflows depending on your environment:

**A) One-click Dev Stack (Windows / WSL 2)**

```powershell
# From the repo root
scripts\start_dev.bat
```

This script launches and hot-reloads the complete stack:
1. **Qdrant** vector database (6333)
2. **GPU-UMAP micro-service** â€“ FastAPI + RAPIDS cuML (8003, Docker)
3. **Ingestion Orchestration API** â€“ FastAPI (8002)
4. **ML Inference API** â€“ FastAPI (8001, Host Python)
5. **Next.js Frontend** â€“ auto-opened at http://localhost:3000
6. **Dev Graph Timeline** â€“ WebGL visualization at http://localhost:3000/dev-graph/timeline

**Note:** The frontend automatically detects and chooses between GPU UMAP (8003) and ML Inference (8001) services.

**B) Manual / Linux / macOS**

```bash
# 1. Start Qdrant vector database
docker compose up -d qdrant_db

# 2. Choose ONE of these clustering options:

# Option A: GPU-accelerated UMAP service (Docker, recommended)
docker compose -f backend/gpu_umap_service/docker-compose.dev.yml up -d --build

# Option B: CPU-only ML inference service (manual Python)
uvicorn backend.ml_inference_fastapi_app.main:app --port 8001 --reload &

# 3. Start ingestion orchestration API
uvicorn backend.ingestion_orchestration_fastapi_app.main:app --port 8002 --reload &

# 4. Start frontend
cd frontend && npm install && npm run dev
```

**Access points:**
* **Frontend** â†’ http://localhost:3000  
* **Ingestion API docs** â†’ http://localhost:8002/docs  
* **GPU-UMAP docs** â†’ http://localhost:8003/docs (if using Docker option)
* **ML Inference docs** â†’ http://localhost:8001/docs (if using manual option)

## ğŸ“ˆ Performance Highlights

- **Sub-second search** across 100K+ images
- **GPU-optimized inference** with 10x speed improvement
- **Concurrent processing** of multiple collections
- **Memory-efficient** model management
- **Real-time latent-space rendering** â€“ 1 k+ point UMAP projection <3 s on consumer GPUs
- **Real-time progress** tracking and updates

## ğŸ“š Documentation

- [Backend Architecture](/backend/ARCHITECTURE.md)
- [Frontend Architecture](/frontend/ARCHITECTURE.md)  
- [Sprint Documentation](/docs/sprints/)
   - [Sprint 11 Overview](/docs/sprints/sprint-11/README.md)
   - [Sprint 11 PRD](/docs/sprints/sprint-11/PRD.md)
   - [Sprint 11 Technical Plan](/docs/sprints/sprint-11/technical-implementation-plan.md)
   - [Sprint 11 Quick Reference](/docs/sprints/sprint-11/QUICK_REFERENCE.md)
   - [CUDA Acceleration Guide](/docs/sprints/sprint-11/CUDA_ACCELERATION_GUIDE.md)
- [API Reference](/backend/ingestion_orchestration_fastapi_app/README.md)

## ğŸ¨ Portfolio Highlights

This project demonstrates:
- **Full-Stack Expertise** - End-to-end application development
- **AI/ML Integration** - Production-ready ML model deployment
- **Modern Architecture** - Microservices, containers, and scalability
- **Performance Optimization** - GPU acceleration and efficient algorithms
- **User Experience** - Intuitive interface design and responsive development

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

*Built with â¤ï¸ showcasing modern full-stack development with AI integration* 
