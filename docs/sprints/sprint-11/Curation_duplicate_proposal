# Proposal: The Vibe Coding Curation & Duplicate Management Suite

**Date:** June 28, 2025  
**Status:** Proposal for Review (Updated)
**Author:** AI Assistant

## 1. Executive Summary

This document proposes a **Curation & Duplicate Management Suite** to ensure the Vibe Coding image database remains a high-quality, pristine, and curated collection of unique assets. As we prepare to ingest a lifetime collection of over 100,000 images, it is critical to implement a robust workflow that prevents new duplicates, provides tools to find existing near-duplicates, and offers a safe, human-supervised method for archival.

The proposed suite is built on two core principles: **zero data loss** and **curator-in-control**. It combines three pillars of functionality:
1.  **Proactive Suppression:** Real-time prevention of byte-for-byte identical duplicates during ingestion.
2.  **Automated Analysis:** An on-demand process to find and group visually similar images already in the database.
3.  **Interactive Curation:** A unified visual interface for reviewing and archiving unwanted images, whether they are exact duplicates, near-duplicates, or simply undesired shots.

**Business Value:**
*   **Database Integrity:** Prevents database pollution, ensuring faster search, lower storage costs, and higher quality results.
*   **Workflow Efficiency:** Automates the tedious and error-prone task of finding and removing duplicate or low-quality files scattered across decades of folders.
*   **Archival Safety:** All operations are non-destructive and reversible, preserving the original files until a curator explicitly confirms their deletion from the archive folders.
*   **Enhanced Curation:** Empowers the user to move beyond simple duplicates and curate the collection based on artistic quality directly from the visual interface.

---

## 2. The Challenge: Curation at Scale

The project faces three distinct but related challenges:

1.  **Exact Duplicates:** The master photo library contains thousands of identical files with different names or in different folders. Ingesting these would pollute the database, skew search results, and waste GPU resources.
2.  **Near-Duplicates:** The collection includes bursts of photos, bracketed exposures, and slightly different compositions of the same subject. While technically unique, these often represent a single artistic idea, and a curator may wish to select only the best one.
3.  **Artistic "Duplicates" & Missed Shots:** The collection also contains numerous unwanted shots (e.g., out-of-focus images, test shots) that are technically unique but add no value. Identifying these manually in a folder tree is impossible.

A solution must address all three issues without ever putting the original files at risk.

---

## 3. The Proposed Solution: A Three-Pillar Curation Suite

We will implement a suite of features that provide a comprehensive, safe, and intuitive curation workflow.

### Pillar 1: Exact Duplicate Management

This pillar focuses on finding and managing files that are byte-for-byte identical.

**1. Proactive, Real-Time Suppression (During Ingestion)**
*   **How it Works:** For every image processed, we compute its SHA-256 hash. Before sending it to the ML service or Qdrant, we perform a quick check (`~1ms`) to see if a point with that `file_hash` already exists in the target collection.
*   **Outcome:** If the hash exists, the new file is **skipped**. It is not processed by the ML service and not added to Qdrant. A record of the skipped file is added to a "Duplicates Found" report for the ingestion job.
*   **Result:** The Qdrant collection remains 100% free of logical duplicates from the start.

**2. Supervised Cleanup Workflow (Post-Ingestion)**
*   **The Problem:** The duplicate files still exist on the physical drive.
*   **The Workflow:**
    1.  **Review:** After an ingest, the UI will present the "Duplicates Found" report, showing thumbnails of the original and the newly found duplicate(s).
    2.  **Select:** The curator can select which files they wish to remove from the source drive.
    3.  **Archive:** Clicking "Archive Exact Duplicates" triggers a backend process that **moves** (not deletes) the selected files into a dedicated folder, `_VibeDuplicates`, located at the root of the ingestion directory. This same-drive move is an instantaneous and safe file system operation.

### Pillar 2: Near-Duplicate Analysis & Curation

This pillar empowers the curator to find, review, and refine the collection based on visual similarity.

**1. Automated Similarity Analysis (The `find-similar` Endpoint)**
*   **How it Works:** A new `POST /api/v1/duplicates/find-similar` endpoint will trigger a background task to scan the collection for visually similar images. The task iterates through points and uses vector similarity search to find groups of near-duplicates (e.g., cosine similarity > 0.98).
*   **Outcome:** This intensive process runs in the background. Its progress can be tracked, and the final report—a list of near-duplicate groups—is stored for review.

**2. Interactive Curation Workflow**
*   **The Workflow:**
    1.  **Review:** A new UI view will present the near-duplicate groups found by the analysis. It will show thumbnails side-by-side, allowing the curator to easily compare them.
    2.  **Select:** The curator selects which images to keep and which to archive.
    3.  **Archive:** Clicking "Archive Selection" uses the same safe archival process described below.

### Pillar 3: General Visual Curation

This pillar provides tools for general collection cleanup using the UMAP Latent Space view.

**1. Visual Identification of Unwanted Images**
*   **How it Works:** Using the UMAP scatterplot, the curator can easily spot and select outliers, clusters of test shots, or blurry images. The existing lasso tool is used to select these points.

**2. The Unified "Archive Selection" Feature**
*   **The Workflow:**
    1.  **Select:** The curator selects one or more points on the UMAP canvas or in the near-duplicate review UI.
    2.  **Archive:** A new "Archive Selection" button becomes active. Clicking it initiates a backend process.
    3.  **Safe Removal:** The backend first takes an **atomic snapshot of the Qdrant collection** for instant rollback. It then **moves** the corresponding original files from their location on the SSD to a `_VibeArchive` folder (at the root of the ingestion directory). Finally, it removes the point records from the live Qdrant collection.

---

## 4. Safety Guarantees: The Curator is Always in Control

This entire suite is designed with the safety of a life's work as its highest priority.

*   **No Automatic Deletion:** The system **never** deletes a file. It only moves files to a designated `_VibeDuplicates` or `_VibeArchive` folder on the same drive, awaiting the curator's final manual review and deletion.
*   **Instantaneous Rollback:** Every curation action that modifies the Qdrant database is preceded by a snapshot. A full restore of the collection's state before the action is a single API call away.
*   **Same-Drive Operations:** By keeping the archive folders on the same physical drive, we use atomic `rename` operations, which are extremely fast and eliminate the risk of file corruption during a copy.
*   **Clear Audit Trail:** Every ingestion job will produce a report of duplicates found, and every archive action will be logged with its corresponding snapshot ID.

---

## 5. Technical Implementation Overview

This feature requires the following new components:

**New Backend API Endpoints (in `duplicates.py` and a new `curation.py` router):**
*   `POST /api/v1/duplicates/find-similar`: Triggers a background task to find visually similar images. Returns a task ID for status polling. This formalizes and correctly implements the stub from `duplicates.py`.
*   `GET /api/v1/duplicates/report/{task_id}`: Retrieves the status and results of a `find-similar` task.
*   `POST /api/v1/duplicates/archive-exact`: Receives a list of file paths (of exact duplicates) and moves them to the `_VibeDuplicates` folder.
*   `POST /api/v1/curation/archive-selection`: Receives a list of Qdrant point IDs (from the near-duplicate UI or UMAP selection), finds their file paths, and moves them to the `_VibeArchive` folder. Triggers a Qdrant snapshot before operating and then deletes the points from the collection.

**Backend Logic Changes:**
*   **Ingestion:** Add the real-time hash check before upserting points to Qdrant.
*   **Job Status:** Enhance the job status model to include the list of found exact duplicates.
*   **`duplicates.py`:** Implement the `find-similar` background task with a robust, performant, and batch-based approach to scan the collection without overloading the server.

**Frontend UI Additions:**
*   **Ingestion Report:** A view to display the list of exact duplicates found, with selection checkboxes and the "Archive Exact Duplicates" action.
*   **Curation View:** A new section in the UI to trigger the similarity analysis and review the resulting groups of near-duplicates.
*   **UMAP View:** An "Archive Selection" button that activates when points are selected.
*   **Confirmation Modals:** Dialogs that clearly explain what will happen and confirm the action with the user.

---

### UI/UX Integration Strategy: A Collection-Centric Approach

To ensure a seamless user experience, the Curation Suite will be integrated directly into the existing collection management interface. This makes curation a core, contextual part of managing a collection, rather than a separate, disconnected tool.

1.  **The Collections Page as a Curation Hub:**
    *   The main `/collections` view will be enhanced to serve as the central hub for all curation activities.
    *   Next to each collection's name and stats, a "Curation Status" will be displayed (e.g., "Not Analyzed", "Analysis in Progress", "Report Ready").
    *   An actions menu (`...`) for each collection will provide direct access to curation tools.

2.  **Targeted, On-Demand Analysis:**
    *   The actions menu for a collection will contain a "**Find Near-Duplicates**" button.
    *   Clicking this triggers the analysis specifically for that collection, providing a clear and intentional workflow.

3.  **Unified Reporting:**
    *   Once an analysis is complete, a "**View Duplicate Report**" link will appear in the collection's action menu, taking the user to the review interface.
    *   The ingestion report, showing *exact* duplicates found, will also be accessible from this same menu, creating a single point of entry for all of a collection's curation-related reports.

This collection-centric model makes the entire process more intuitive, discoverable, and aligned with the user's mental model of managing their image sets.

---

## 6. Conclusion

The proposed Curation & Duplicate Management Suite is a critical feature that transforms the project from a simple ingestion pipeline into a professional-grade archival and exploration tool. It directly addresses the core user needs for a clean, curated, and reliable database by tackling exact duplicates, near-duplicates, and general artistic selection. By prioritizing safety through human supervision and non-destructive operations, we can confidently proceed with ingesting the full collection, knowing that the integrity of both the source material and the resulting database is guaranteed.